{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4e960023-6a08-4451-a1c7-a070f129b2ec",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e78b3208-a45d-4fc0-a08a-5b77ef33ad85",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "#Create session\n",
    "spark = SparkSession.builder.appName(\"IPL DATA ANALYSIS\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d07f6737-dbec-4eed-81fc-f98b9f77264e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "53e4383d-6679-450c-9b2c-689367f8a221",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Set AWS credentials if AWS S3 bucket is not publicly accessible\n",
    "spark._jsc.hadoopConfiguration().set(\"fs.s3a.access.key\", \"AKIA2S2Y36KFBTRFY3PX\")\n",
    "spark._jsc.hadoopConfiguration().set(\"fs.s3a.secret.key\", \"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\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9fc94a9a-901c-4920-9e79-d684c53ed9f9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Import packages to build the schema (Recommened as inferSchema may not get accurate data type)\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, BooleanType, DateType, StringType, DecimalType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c38ad3ab-4a6c-4d67-a281-e7d67e977d70",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Create the schema for ball_by_ball data\n",
    "ball_by_ball_schema = StructType([\n",
    "    StructField(\"match_id\",IntegerType(),True),\n",
    "    StructField(\"over_id\",IntegerType(),True),\n",
    "    StructField(\"ball_id\",IntegerType(),True),\n",
    "    StructField(\"innings_no\", IntegerType(), True),\n",
    "    StructField(\"team_batting\", StringType(), True),\n",
    "    StructField(\"team_bowling\", StringType(), True),\n",
    "    StructField(\"striker_batting_position\", IntegerType(), True),\n",
    "    StructField(\"extra_type\", StringType(), True),\n",
    "    StructField(\"runs_scored\", IntegerType(), True),\n",
    "    StructField(\"extra_runs\", IntegerType(), True),\n",
    "    StructField(\"wides\", IntegerType(), True),\n",
    "    StructField(\"legbyes\", IntegerType(), True),\n",
    "    StructField(\"byes\", IntegerType(), True),\n",
    "    StructField(\"noballs\", IntegerType(), True),\n",
    "    StructField(\"penalty\", IntegerType(), True),\n",
    "    StructField(\"bowler_extras\", IntegerType(), True),\n",
    "    StructField(\"out_type\", StringType(), True),\n",
    "    StructField(\"caught\", BooleanType(), True),\n",
    "    StructField(\"bowled\", BooleanType(), True),\n",
    "    StructField(\"run_out\", BooleanType(), True),\n",
    "    StructField(\"lbw\", BooleanType(), True),\n",
    "    StructField(\"retired_hurt\", BooleanType(), True),\n",
    "    StructField(\"stumped\", BooleanType(), True),\n",
    "    StructField(\"caught_and_bowled\", BooleanType(), True),\n",
    "    StructField(\"hit_wicket\", BooleanType(), True),\n",
    "    StructField(\"obstructingfeild\", BooleanType(), True),\n",
    "    StructField(\"bowler_wicket\", BooleanType(), True),\n",
    "    StructField(\"match_date\", DateType(), True),\n",
    "    StructField(\"season\", IntegerType(), True),\n",
    "    StructField(\"striker\", IntegerType(), True),\n",
    "    StructField(\"non_striker\", IntegerType(), True),\n",
    "    StructField(\"bowler\", IntegerType(), True),\n",
    "    StructField(\"player_out\", IntegerType(), True),\n",
    "    StructField(\"fielders\", IntegerType(), True),\n",
    "    StructField(\"striker_match_sk\", IntegerType(), True),\n",
    "    StructField(\"strikersk\", IntegerType(), True),\n",
    "    StructField(\"nonstriker_match_sk\", IntegerType(), True),\n",
    "    StructField(\"nonstriker_sk\", IntegerType(), True),\n",
    "    StructField(\"fielder_match_sk\", IntegerType(), True),\n",
    "    StructField(\"fielder_sk\", IntegerType(), True),\n",
    "    StructField(\"bowler_match_sk\", IntegerType(), True),\n",
    "    StructField(\"bowler_sk\", IntegerType(), True),\n",
    "    StructField(\"playerout_match_sk\", IntegerType(), True),\n",
    "    StructField(\"battingteam_sk\", IntegerType(), True),\n",
    "    StructField(\"bowlingteam_sk\", IntegerType(), True),\n",
    "    StructField(\"keeper_catch\", BooleanType(), True),\n",
    "    StructField(\"player_out_sk\", IntegerType(), True),\n",
    "    StructField(\"matchdatesk\", DateType(), True)\n",
    "])\n",
    "\n",
    "#Read the data with above schema\n",
    "ball_by_ball_df = spark.read.schema(ball_by_ball_schema).format('csv').option(\"header\",\"true\").load(\"s3://atish-ipl-data-analysis/raghu543_ipl-data-till-2017/2025-09-07/ball_by_ball.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "62544fba-1c2f-47dc-a820-0de9d6946a78",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "ball_by_ball_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3c442af4-9b7a-4560-bc5e-75e9d361a6f9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "match_schema = StructType([\n",
    "    StructField(\"match_sk\",IntegerType(),True),\n",
    "    StructField(\"match_id\", IntegerType(), True),\n",
    "    StructField(\"team1\", StringType(), True),\n",
    "    StructField(\"team2\", StringType(), True),\n",
    "    StructField(\"match_date\", DateType(), True),\n",
    "    StructField(\"season_year\", IntegerType(), True),\n",
    "    StructField(\"venue_name\", StringType(), True),\n",
    "    StructField(\"city_name\", StringType(), True),\n",
    "    StructField(\"country_name\", StringType(), True),\n",
    "    StructField(\"toss_winner\", StringType(), True),\n",
    "    StructField(\"match_winner\", StringType(), True),\n",
    "    StructField(\"toss_name\", StringType(), True),\n",
    "    StructField(\"win_type\", StringType(), True),\n",
    "    StructField(\"outcome_type\", StringType(), True),\n",
    "    StructField(\"manofmach\", StringType(), True),\n",
    "    StructField(\"win_margin\", IntegerType(), True),\n",
    "    StructField(\"country_id\", IntegerType(), True)\n",
    "])\n",
    "\n",
    "#Read match data\n",
    "match_df = spark.read.schema(match_schema).format('csv').option(\"header\",\"true\").load(\"s3://atish-ipl-data-analysis/raghu543_ipl-data-till-2017/2025-09-07/match.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6928bbf3-ec1c-4291-8888-daf033a34618",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "player_schema = StructType([\n",
    "    StructField(\"player_sk\", IntegerType(), True),\n",
    "    StructField(\"player_id\", IntegerType(), True),\n",
    "    StructField(\"player_name\", StringType(), True),\n",
    "    StructField(\"dob\", DateType(), True),\n",
    "    StructField(\"batting_hand\", StringType(), True),\n",
    "    StructField(\"bowling_skill\", StringType(), True),\n",
    "    StructField(\"country_name\", StringType(), True)\n",
    "])\n",
    "\n",
    "player_df = spark.read.schema(player_schema).format(\"csv\").option(\"header\",\"true\").load(\"s3://atish-ipl-data-analysis/raghu543_ipl-data-till-2017/2025-09-07/player.csv\")\n",
    "\n",
    "player_df.show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "63c779ca-44cd-4b50-90e8-8e9e2da3f4f9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "player_match_schema = StructType([\n",
    "    StructField(\"player_match_sk\", IntegerType(), True),\n",
    "    StructField(\"playermatch_key\", DecimalType(), True),\n",
    "    StructField(\"match_id\", IntegerType(), True),\n",
    "    StructField(\"player_id\", IntegerType(), True),\n",
    "    StructField(\"player_name\", StringType(), True),\n",
    "    StructField(\"dob\", DateType(), True),\n",
    "    StructField(\"batting_hand\", StringType(), True),\n",
    "    StructField(\"bowling_skill\", StringType(), True),\n",
    "    StructField(\"country_name\", StringType(), True),\n",
    "    StructField(\"role_desc\", StringType(), True),\n",
    "    StructField(\"player_team\", StringType(), True),\n",
    "    StructField(\"opposit_team\", StringType(), True),\n",
    "    StructField(\"season_year\", IntegerType(), True),\n",
    "    StructField(\"is_manofthematch\", BooleanType(), True),\n",
    "    StructField(\"age_as_on_match\", IntegerType(), True),\n",
    "    StructField(\"isplayers_team_won\", BooleanType(), True),\n",
    "    StructField(\"batting_status\", StringType(), True),\n",
    "    StructField(\"bowling_status\", StringType(), True),\n",
    "    StructField(\"player_captain\", StringType(), True),\n",
    "    StructField(\"opposit_captain\", StringType(), True),\n",
    "    StructField(\"player_keeper\", StringType(), True),\n",
    "    StructField(\"opposit_keeper\", StringType(), True)\n",
    "])\n",
    "\n",
    "player_match_df = spark.read.schema(player_match_schema).format(\"csv\").option(\"header\",\"true\").load(\"s3://atish-ipl-data-analysis/raghu543_ipl-data-till-2017/2025-09-07/player_match.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c51def79-93da-4ee1-a0f2-2738aaf4892c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "team_schema = StructType([\n",
    "    StructField(\"team_sk\", IntegerType(), True),\n",
    "    StructField(\"team_id\", IntegerType(), True),\n",
    "    StructField(\"team_name\", StringType(), True)\n",
    "])\n",
    "\n",
    "team_df = spark.read.schema(team_schema).format(\"csv\").option(\"header\",\"true\").load(\"s3://atish-ipl-data-analysis/raghu543_ipl-data-till-2017/2025-09-07/team.csv\")\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "71003894-179c-45df-b1a4-573ca8622419",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Transformation Logic\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "#Filter: exclude the delveries with extras(wides and no balls)\n",
    "ball_by_ball_no_extras_df = ball_by_ball_df.filter(((col(\"wides\")) == 0) & ((col(\"noballs\")) == 0))\n",
    "\n",
    "#Aggregation: Calcuate the total and average runs scored per match and innings\n",
    "total_and_avg_runs = ball_by_ball_df.groupBy(\"match_id\",\"innings_no\").agg(\n",
    "    sum(\"runs_scored\").alias(\"total_runs\"),\n",
    "    avg(\"runs_scored\").alias(\"avg_runs\"),\n",
    ").orderBy(\"match_id\",\"innings_no\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "248fc192-b214-40ff-b397-9e30807b5447",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Window Function\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "#Calcualte the running total of runs in each match per over\n",
    "#Create a window for each match and innings on over\n",
    "windowspec = Window.partitionBy(\"match_id\",\"innings_no\").orderBy(\"over_id\")\n",
    "\n",
    "#add new column of running_totals in df\n",
    "ball_by_ball_df = ball_by_ball_df.withColumn(\n",
    "                                                \"running_total\",sum(\"runs_scored\").over(windowspec)\n",
    "                                             )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ee89cdb7-6e53-495d-99d6-06ea096d58a4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Conditional Column:\n",
    "#Flag an high impact delivery such as wicket ball or with more than six runs scored\n",
    "ball_by_ball_df = ball_by_ball_df.withColumn(\n",
    "                                                \"high_impact\",\n",
    "                                                when(\n",
    "                                                    ((col(\"runs_scored\") + col(\"extra_runs\")) >= 6) | (col(\"bowler_wicket\")== True), True).otherwise(False)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "991dbfb0-78eb-403f-8146-4bbb1f9ff0a7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "ball_by_ball_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "828fa0f7-d067-48c2-a20b-44a491c8630a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Transformation on match_df\n",
    "#Extracting year, month and dayofmonth from match_date\n",
    "match_df = match_df.withColumn(\"year\",year(\"match_date\"))\n",
    "match_df = match_df.withColumn(\"month\",month(\"match_date\"))\n",
    "match_df = match_df.withColumn(\"day\",dayofmonth(\"match_date\"))\n",
    "\n",
    "#Categorizing the win_margin\n",
    "match_df = match_df.withColumn(\n",
    "            \"win_margin_category\",\n",
    "            when(\n",
    "                (col(\"win_margin\") >= 100),\"High\"\n",
    "            )\n",
    "            .when(\n",
    "                (col(\"win_margin\")>=50) & (col(\"win_margin\") < 100),\"Medium\"\n",
    "            )\n",
    "            .otherwise(\"Low\")\n",
    ")\n",
    "\n",
    "#Analyzing whether toss winner is the match winner\n",
    "match_df = match_df.withColumn(\n",
    "                \"toss_match_winner\",\n",
    "                when(\n",
    "                    col(\"toss_winner\")==col(\"match_winner\"),\"True\"\n",
    "                )\n",
    "                .otherwise(\"False\")\n",
    ")\n",
    "\n",
    "match_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4d293762-9c31-4a55-845b-27bc93d42f91",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Transformation on player_df\n",
    "\n",
    "#Normalizing and clean player names\n",
    "#player_df = player_df.withColumn(\n",
    "#                        \"player_name\",lower(regexp_replace(\"player_name\", \"[^a-zA-Z0-9]\", \"\"))    \n",
    "#)\n",
    "\n",
    "#Fill Null Values\n",
    "player_df = player_df.na.fill(\n",
    "                        {\"batting_hand\":\"unknown\",\"bowling_skill\":\"unknown\"}\n",
    ")\n",
    "\n",
    "#Categorizing the player by cleaning batting_hand column and creating batting_style\n",
    "player_df = player_df.withColumn(\n",
    "                        \"batting_style\",\n",
    "                        when(\n",
    "                            lower(col(\"batting_hand\")).contains(\"left\"),\"Left-Handed\"\n",
    "                        )\n",
    "                        .otherwise(\"Right-Handed\")\n",
    ")\n",
    "\n",
    "player_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "18c1b181-bd09-432b-a0ee-c5d74ec4d72a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Transformation on player_match_df\n",
    "\n",
    "# Add a 'veteran_status' column based on player age\n",
    "player_match_df = player_match_df.withColumn(\n",
    "                        \"veteran_status\",\n",
    "                        when(col(\"age_as_on_match\") >= 35, \"Veteran\").otherwise(\"Non-Veteran\")\n",
    ")\n",
    "\n",
    "#Year since debut column\n",
    "player_match_df = player_match_df.withColumn(\n",
    "                                \"years_since_debut\",\n",
    "                                (year(current_date()) - col(\"season_year\"))\n",
    ")\n",
    "\n",
    "player_match_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "276ce232-2281-4825-87ad-9c2c36a127a8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Working with SparkSQL\n",
    "\n",
    "#First creating a temp view\n",
    "ball_by_ball_df.createOrReplaceTempView(\"ball_by_ball\")\n",
    "match_df.createOrReplaceTempView(\"match\")\n",
    "team_df.createOrReplaceTempView(\"team\")\n",
    "player_df.createOrReplaceTempView(\"player\")\n",
    "player_match_df.createOrReplaceTempView(\"player_match\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "96073f29-12fa-42a6-8d19-cff7f34f4373",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "ball_by_ball_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ff29f2da-7ee4-4084-8a61-2ea7c9c8dab3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "top_run_scorers_per_season = spark.sql(\"\"\"\n",
    "SELECT\n",
    "    p.player_name,\n",
    "    m.season_year,\n",
    "    SUM(b.runs_scored) AS total_runs\n",
    "FROM ball_by_ball AS b\n",
    "JOIN match AS m\n",
    "    ON b.match_id = m.match_id\n",
    "JOIN player_match AS pm\n",
    "    ON pm.match_id = m.match_id AND pm.player_id = b.striker\n",
    "JOIN player AS p\n",
    "    ON pm.player_id = p.player_id\n",
    "GROUP BY p.player_name, m.season_year\n",
    "ORDER BY m.season_year, total_runs DESC                                    \n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d5e3dae4-f2d3-407b-9150-432d2fff248a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "top_run_scorers_per_season.show(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3e0b658b-df75-499a-83fc-bb0cdebc59dc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "top_run_scorer_all_time = spark.sql(\"\"\"\n",
    "                                    SELECT\n",
    "                                        p.player_name,\n",
    "                                        SUM(b.runs_scored) AS total_runs\n",
    "                                    FROM ball_by_ball AS b\n",
    "                                    JOIN match AS m\n",
    "                                        ON b.match_id = m.match_id\n",
    "                                    JOIN player_match AS pm\n",
    "                                        ON pm.match_id = m.match_id AND pm.player_id = b.striker\n",
    "                                    JOIN player AS p\n",
    "                                        ON pm.player_id = p.player_id\n",
    "                                    GROUP BY p.player_name\n",
    "                                    ORDER BY total_runs DESC\n",
    "                                    \"\"\"\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2b44d6ef-f132-4c23-8b8d-35126874cf95",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "top_run_scorer_all_time.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f5806f20-847f-4038-bcf9-c4afea184bfb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "leading_wicket_taker_per_season = spark.sql(\"\"\"\n",
    "                                            SELECT\n",
    "                                                p.player_name,\n",
    "                                                m.season_year,\n",
    "                                                SUM(CAST(b.bowler_wicket AS INT)) AS total_wickets\n",
    "                                            FROM ball_by_ball AS b\n",
    "                                            JOIN match AS m\n",
    "                                                ON b.match_id = m.match_id\n",
    "                                            JOIN player_match AS pm\n",
    "                                                ON pm.match_id = m.match_id AND pm.player_id = b.bowler\n",
    "                                            JOIN player AS p\n",
    "                                                ON p.player_id = pm.player_id\n",
    "                                            GROUP BY p.player_name, m.season_year\n",
    "                                            ORDER BY m.season_year, total_wickets DESC\n",
    "                                            \"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7a4353c9-1f81-47c5-a360-74bf6fec4854",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "leading_wicket_taker_per_season.show(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cc0b0bdb-eef8-4890-afc6-2f56dca7f5ad",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "leading_wicker_taker_all_time = spark.sql(\"\"\"\n",
    "                                            SELECT\n",
    "                                                p.player_name,\n",
    "                                                SUM(CAST(b.bowler_wicket AS INT)) AS total_wickets\n",
    "                                            FROM ball_by_ball AS b\n",
    "                                            JOIN match AS m\n",
    "                                                ON b.match_id = m.match_id\n",
    "                                            JOIN player_match AS pm\n",
    "                                                ON pm.match_id = m.match_id AND pm.player_id = b.bowler\n",
    "                                            JOIN player AS p\n",
    "                                                ON p.player_id = pm.player_id\n",
    "                                            GROUP BY p.player_name\n",
    "                                            ORDER BY total_wickets DESC\n",
    "                                          \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b006f5aa-7bb4-4842-8dda-0ff7366de708",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "leading_wicker_taker_all_time.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "56a332e5-664f-4db5-a3cb-43db14b1a4f6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "match_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7d6c43af-e9e2-48bc-b715-00dde47f12b0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Toss and Match Win by Venue\n",
    "toss_and_win_by_venue = spark.sql(\"\"\"\n",
    "                            SELECT \n",
    "                                DISTINCT(venue_name) AS venue,\n",
    "                                COUNT(match_id) AS total_matches,\n",
    "                                SUM(\n",
    "                                    CASE WHEN toss_match_winner = True THEN 1 ELSE 0 END\n",
    "                                ) AS toss_match_winner_count,\n",
    "                                ROUND(SUM(CASE WHEN toss_match_winner = True THEN 1 ELSE 0 END)/COUNT(match_id),2) AS toss_match_win_pct  \n",
    "                            FROM match AS m\n",
    "                            GROUP BY venue\n",
    "                            ORDER BY total_matches DESC, toss_match_win_pct DESC\n",
    "\"\"\")\n",
    "\n",
    "toss_and_win_by_venue.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6e39df7f-4bc3-4fe4-8d37-c0d437d3ae51",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Type of Win by Venue\n",
    "type_of_wins_by_venue = spark.sql(\"\"\"\n",
    "                                  SELECT\n",
    "                                      DISTINCT(venue_name) AS venue,\n",
    "                                      COUNT(match_id) AS total_matches,\n",
    "                                      SUM(\n",
    "                                          CASE WHEN win_type = 'runs' THEN 1 ELSE 0 END\n",
    "                                      ) AS first_bat_win_count,\n",
    "                                      ROUND(SUM(CASE WHEN win_type = 'runs' THEN 1 ELSE 0 END)/COUNT(match_id),2) AS first_bat_win_pct,\n",
    "                                      SUM(\n",
    "                                          CASE WHEN win_type = 'wickets' THEN 1 ELSE 0 END\n",
    "                                          ) AS chase_win_count,\n",
    "                                      ROUND(SUM(CASE WHEN win_type = 'wickets' THEN 1 ELSE 0 END)/COUNT(match_id),2) AS chase_win_pct\n",
    "                                  FROM match\n",
    "                                  GROUP BY venue\n",
    "                                  ORDER BY total_matches DESC\n",
    "                                  \"\"\")\n",
    "type_of_wins_by_venue.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "18d9c127-fdf3-49bd-8c94-25ce32b94910",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "team_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c1700010-e570-48b2-b050-33644f6a458e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Toss and Match Win By Team\n",
    "toss_and_win_by_team = spark.sql(\"\"\"\n",
    "                            SELECT  \n",
    "                                t.team_name AS team,\n",
    "                                COUNT(m.match_id) AS total_matches,\n",
    "                                SUM(\n",
    "                                    CASE WHEN t.team_name = m.match_winner THEN 1 ELSE 0 END\n",
    "                                ) AS match_wins,\n",
    "                                SUM(\n",
    "                                    CASE WHEN t.team_name = m.toss_winner THEN 1 ELSE 0 END\n",
    "                                ) AS toss_wins,\n",
    "                                SUM(\n",
    "                                    CASE WHEN t.team_name = m.toss_winner AND t.team_name = m.match_winner THEN 1 ELSE 0 END\n",
    "                                ) AS toss_match_win,\n",
    "                                ROUND(\n",
    "                                    SUM(CASE WHEN t.team_name = m.match_winner THEN 1 ELSE 0 END)/ COUNT(m.match_id), 2\n",
    "                                ) AS win_pct,\n",
    "                                ROUND(SUM(\n",
    "                                    CASE WHEN t.team_name = m.toss_winner AND t.team_name = m.match_winner THEN 1 ELSE 0 END)\n",
    "                                / SUM(\n",
    "                                    CASE WHEN t.team_name = m.match_winner THEN 1 ELSE 0 END),2) AS toss_match_win_pct\n",
    "                            FROM match AS m\n",
    "                            JOIN team AS t\n",
    "                                ON m.team1 = t.team_name OR m.team2 = t.team_name\n",
    "                            GROUP BY t.team_name\n",
    "                            ORDER BY toss_match_win_pct DESC, win_pct DESC\n",
    "\"\"\")\n",
    "\n",
    "toss_and_win_by_team.show()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "3"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "IPL_DATA_ANALYSIS_SPARK",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
